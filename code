# -*- coding: utf-8 -*-
"""cse425.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBMcZBgCIoJvGqvslj4AF54SukL7HidH
"""

!pip -q install librosa soundfile umap-learn

import os

os.makedirs("data/audio", exist_ok=True)
os.makedirs("data/audio_processed", exist_ok=True)
os.makedirs("results/latent_visualization", exist_ok=True)

print("Folders created.")

from google.colab import files
uploaded = files.upload()

import shutil, os

for fname in uploaded.keys():
    if fname.lower().endswith(".wav"):
        shutil.move(fname, os.path.join("data/audio", fname))

print("Moved WAV files to data/audio/")

import os
import pandas as pd

AUDIO_DIR = "data/audio"
OUTPUT_CSV = "data/metadata.csv"

rows = []
wav_files = sorted([f for f in os.listdir(AUDIO_DIR) if f.lower().endswith(".wav")])

for idx, filename in enumerate(wav_files, start=1):
    track_id = f"{idx:03d}"
    audio_path = os.path.join(AUDIO_DIR, filename).replace("\\", "/")
    rows.append({"track_id": track_id, "audio_path": audio_path, "language": "Unknown"})

df = pd.DataFrame(rows)
df.to_csv(OUTPUT_CSV, index=False)

df.head(), f"✅ Saved {len(df)} rows to {OUTPUT_CSV}"

import pandas as pd
import librosa
import soundfile as sf
import numpy as np
import os

TARGET_SR = 22050
DURATION_SEC = 30
OUT_DIR = "data/audio_processed"
os.makedirs(OUT_DIR, exist_ok=True)

df = pd.read_csv("data/metadata.csv")

for _, row in df.iterrows():
    track_id = row["track_id"]
    in_path = row["audio_path"]

    y, sr = librosa.load(in_path, sr=TARGET_SR, mono=True)
    target_len = TARGET_SR * DURATION_SEC

    if len(y) > target_len:
        y = y[:target_len]
    else:
        y = librosa.util.fix_length(y, size=target_len)

    out_path = os.path.join(OUT_DIR, f"{track_id}.wav")
    sf.write(out_path, y, TARGET_SR)

print("✅ Done preprocessing. Files saved in data/audio_processed/")

import numpy as np
import pandas as pd
import librosa
import os

PROC_DIR = "data/audio_processed"
N_MFCC = 20

df = pd.read_csv("data/metadata.csv")

X = []
ids = []

for _, row in df.iterrows():
    track_id = row["track_id"]
    path = os.path.join(PROC_DIR, f"{track_id}.wav")

    y, sr = librosa.load(path, sr=None, mono=True)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)

    feat = np.concatenate([mfcc.mean(axis=1), mfcc.std(axis=1)])  # 40-dim
    X.append(feat)
    ids.append(track_id)

X = np.array(X, dtype=np.float32)

np.save("data/features.npy", X)
pd.DataFrame({"track_id": ids}).to_csv("data/features_track_ids.csv", index=False)

print("✅ Features saved:", X.shape)

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score

X = np.load("data/features.npy")
ids = pd.read_csv("data/features_track_ids.csv")["track_id"].tolist()

X_scaled = StandardScaler().fit_transform(X)

# Ensure there are enough samples for PCA and clustering
if X_scaled.shape[0] < 2:
    print("Not enough samples for PCA and clustering. Need at least 2 samples.")
    baseline_df = pd.DataFrame(columns=["method", "k", "silhouette", "calinski_harabasz", "pca_components"])
else:
    # N_PCA must be less than or equal to the number of samples and features
    N_PCA = min(20, X_scaled.shape[1], X_scaled.shape[0])
    X_pca = PCA(n_components=N_PCA, random_state=42).fit_transform(X_scaled)

    rows = []
    # Adjust the range for k, as KMeans requires n_clusters <= n_samples
    max_k_for_kmeans = min(10, X_scaled.shape[0])

    # Iterate for k values that are valid for KMeans and silhouette score
    # Silhouette score requires at least 2 clusters for n_samples > 1.
    # KMeans requires n_clusters <= n_samples.
    start_k = max(2, 4) # Ensure k starts from at least 2 for silhouette
    end_k = max_k_for_kmeans # Ensure k does not exceed number of samples

    if end_k < start_k:
        print("Not enough samples for multiple clusters (k >= 2).")
        # If we cannot even run for k=2, then no valid rows will be appended
    else:
        for k in range(start_k, end_k + 1):
            labels = KMeans(n_clusters=k, random_state=42, n_init=10).fit_predict(X_pca)
            rows.append({
                "method": "PCA+KMeans",
                "k": k,
                "silhouette": silhouette_score(X_pca, labels),
                "calinski_harabasz": calinski_harabasz_score(X_pca, labels),
                "pca_components": N_PCA
            })

    baseline_df = pd.DataFrame(rows).sort_values("silhouette", ascending=False)

baseline_df.to_csv("results/baseline_metrics.csv", index=False)

baseline_df.head()

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.preprocessing import StandardScaler

# -----------------------
# Config (safe defaults)
# -----------------------
SEED = 42
BATCH_SIZE = 32
EPOCHS = 80
LR = 1e-3
LATENT_DIM = 16
H1, H2 = 128, 64  # hidden sizes

os.makedirs("results", exist_ok=True)

# Reproducibility
torch.manual_seed(SEED)
np.random.seed(SEED)

# -----------------------
# Load and standardize X
# -----------------------
X = np.load("data/features.npy").astype(np.float32)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X).astype(np.float32)

# Save scaler parameters so you can reuse later (important!)
np.save("results/feature_scaler_mean.npy", scaler.mean_)
np.save("results/feature_scaler_scale.npy", scaler.scale_)

X_tensor = torch.from_numpy(X_scaled)
dataset = TensorDataset(X_tensor)

# Train/val split
n_total = len(dataset)
n_val = max(1, int(0.2 * n_total))
n_train = n_total - n_val

train_ds, val_ds = random_split(
    dataset,
    [n_train, n_val],
    generator=torch.Generator().manual_seed(SEED)
)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_dim = X_scaled.shape[1]

print("Device:", device)
print("X shape:", X_scaled.shape, "| Train:", n_train, "| Val:", n_val)

# -----------------------
# Define VAE
# -----------------------
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim, h1=128, h2=64):
        super().__init__()
        self.enc = nn.Sequential(
            nn.Linear(input_dim, h1),
            nn.ReLU(),
            nn.Linear(h1, h2),
            nn.ReLU()
        )
        self.mu = nn.Linear(h2, latent_dim)
        self.logvar = nn.Linear(h2, latent_dim)

        self.dec = nn.Sequential(
            nn.Linear(latent_dim, h2),
            nn.ReLU(),
            nn.Linear(h2, h1),
            nn.ReLU(),
            nn.Linear(h1, input_dim)
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        h = self.enc(x)
        mu = self.mu(h)
        logvar = self.logvar(h)
        z = self.reparameterize(mu, logvar)
        x_hat = self.dec(z)
        return x_hat, mu, logvar

def vae_loss(x, x_hat, mu, logvar):
    recon = nn.functional.mse_loss(x_hat, x, reduction="mean")
    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon + kl, recon, kl

model = VAE(input_dim=input_dim, latent_dim=LATENT_DIM, h1=H1, h2=H2).to(device)
opt = torch.optim.Adam(model.parameters(), lr=LR)

# -----------------------
# Train
# -----------------------
best_val = float("inf")
history = []

for epoch in range(1, EPOCHS + 1):
    model.train()
    tr_loss = tr_recon = tr_kl = 0.0

    for (xb,) in train_loader:
        xb = xb.to(device)
        opt.zero_grad()
        x_hat, mu, logvar = model(xb)
        loss, recon, kl = vae_loss(xb, x_hat, mu, logvar)
        loss.backward()
        opt.step()

        tr_loss += loss.item() * len(xb)
        tr_recon += recon.item() * len(xb)
        tr_kl += kl.item() * len(xb)

    tr_loss /= n_train
    tr_recon /= n_train
    tr_kl /= n_train

    model.eval()
    va_loss = va_recon = va_kl = 0.0
    with torch.no_grad():
        for (xb,) in val_loader:
            xb = xb.to(device)
            x_hat, mu, logvar = model(xb)
            loss, recon, kl = vae_loss(xb, x_hat, mu, logvar)

            va_loss += loss.item() * len(xb)
            va_recon += recon.item() * len(xb)
            va_kl += kl.item() * len(xb)

    va_loss /= n_val
    va_recon /= n_val
    va_kl /= n_val

    history.append([epoch, tr_loss, tr_recon, tr_kl, va_loss, va_recon, va_kl])

    # Save best model
    if va_loss < best_val:
        best_val = va_loss
        torch.save(model.state_dict(), "results/vae_model.pt")

    if epoch == 1 or epoch % 10 == 0:
        print(f"Epoch {epoch:03d} | train {tr_loss:.4f} (recon {tr_recon:.4f}, kl {tr_kl:.4f}) "
              f"| val {va_loss:.4f} (recon {va_recon:.4f}, kl {va_kl:.4f})")

# Save training log
hist_df = pd.DataFrame(history, columns=[
    "epoch", "train_loss", "train_recon", "train_kl", "val_loss", "val_recon", "val_kl"
])
hist_df.to_csv("results/vae_training_log.csv", index=False)

print("✅ Saved results/vae_model.pt and results/vae_training_log.csv")
print("Best val loss:", best_val)

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn

# -----------------------
# Config (must match Step 8)
# -----------------------
LATENT_DIM = 16
H1, H2 = 128, 64

os.makedirs("results", exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# -----------------------
# Load features + track ids
# -----------------------
X = np.load("data/features.npy").astype(np.float32)
track_ids = pd.read_csv("data/features_track_ids.csv")["track_id"].tolist()

# -----------------------
# Load scaler params saved in Step 8
# -----------------------
mean = np.load("results/feature_scaler_mean.npy").astype(np.float32)
scale = np.load("results/feature_scaler_scale.npy").astype(np.float32)

X_scaled = (X - mean) / scale
X_tensor = torch.from_numpy(X_scaled).to(device)

# -----------------------
# Define VAE encoder (same architecture as Step 8)
# -----------------------
class VAE_Encoder(nn.Module):
    def __init__(self, input_dim, latent_dim, h1=128, h2=64):
        super().__init__()
        self.enc = nn.Sequential(
            nn.Linear(input_dim, h1),
            nn.ReLU(),
            nn.Linear(h1, h2),
            nn.ReLU()
        )
        self.mu = nn.Linear(h2, latent_dim)
        self.logvar = nn.Linear(h2, latent_dim)

    def forward(self, x):
        h = self.enc(x)
        mu = self.mu(h)
        logvar = self.logvar(h)
        return mu, logvar

input_dim = X.shape[1]
encoder = VAE_Encoder(input_dim=input_dim, latent_dim=LATENT_DIM, h1=H1, h2=H2).to(device)

# Load weights from trained VAE (Step 8)
state = torch.load("results/vae_model.pt", map_location=device)
encoder.load_state_dict(state, strict=False)  # strict=False allows loading only matching parts
encoder.eval()

# -----------------------
# Extract latent vectors (use mu)
# -----------------------
with torch.no_grad():
    mu, _ = encoder(X_tensor)
    Z = mu.cpu().numpy()

print("Latent vectors shape:", Z.shape)

# Save outputs
np.save("results/latent_vectors.npy", Z)

latent_df = pd.DataFrame(Z, columns=[f"z{i}" for i in range(Z.shape[1])])
latent_df.insert(0, "track_id", track_ids)
latent_df.to_csv("results/latent_vectors.csv", index=False)

print("✅ Saved results/latent_vectors.npy and results/latent_vectors.csv")
latent_df.head()

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score

os.makedirs("results", exist_ok=True)

# Load latent vectors + track ids
Z = np.load("results/latent_vectors.npy")
track_ids = pd.read_csv("data/features_track_ids.csv")["track_id"].tolist()

# Standardize latent vectors
Zs = StandardScaler().fit_transform(Z)

rows = []
k_values = range(4, 11)

for k in k_values:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = km.fit_predict(Zs)

    sil = silhouette_score(Zs, labels)
    ch = calinski_harabasz_score(Zs, labels)

    rows.append({
        "method": "VAE+KMeans",
        "k": k,
        "silhouette": sil,
        "calinski_harabasz": ch
    })

    # Save labels per k
    out_labels = pd.DataFrame({"track_id": track_ids, "cluster": labels})
    out_labels.to_csv(f"results/vae_kmeans_k{k}_labels.csv", index=False)

vae_metrics = pd.DataFrame(rows).sort_values("silhouette", ascending=False)
vae_metrics.to_csv("results/vae_metrics.csv", index=False)

print("✅ Saved results/vae_metrics.csv and cluster label files for each k")
vae_metrics.head(10)

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE

os.makedirs("results/latent_visualization", exist_ok=True)

# Load latent vectors
Z = np.load("results/latent_vectors.npy")
Zs = StandardScaler().fit_transform(Z)

# Pick best k from silhouette
metrics = pd.read_csv("results/vae_metrics.csv").sort_values("silhouette", ascending=False)
best_k = int(metrics.iloc[0]["k"])
print("Best k (by silhouette):", best_k)

# Load cluster labels for best k
labels_path = f"results/vae_kmeans_k{best_k}_labels.csv"
labels = pd.read_csv(labels_path)["cluster"].values
print("Loaded labels from:", labels_path)

# Try UMAP first; if not available, fall back to t-SNE
use_umap = False
try:
    import umap
    reducer = umap.UMAP(n_components=2, random_state=42)
    emb = reducer.fit_transform(Zs)
    use_umap = True
except Exception as e:
    print("UMAP not available or failed, using t-SNE instead. Error:", e)
    perplexity = min(30, max(5, len(Zs)//5))
    emb = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(Zs)

# Plot
plt.figure(figsize=(7, 5))
plt.scatter(emb[:, 0], emb[:, 1], c=labels, s=18)
plt.title(f"{'UMAP' if use_umap else 't-SNE'} visualization of VAE latent space (k={best_k})")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")

out_path = f"results/latent_visualization/vae_{'umap' if use_umap else 'tsne'}_k{best_k}.png"
plt.savefig(out_path, dpi=200, bbox_inches="tight")
plt.show()

print("✅ Saved plot to:", out_path)

import pandas as pd
import os

os.makedirs("results", exist_ok=True)

# Load metric files
baseline = pd.read_csv("results/baseline_metrics.csv")
vae = pd.read_csv("results/vae_metrics.csv")

# Pick best k by silhouette (you can also pick by CH if you want)
best_baseline = baseline.sort_values("silhouette", ascending=False).iloc[0]
best_vae = vae.sort_values("silhouette", ascending=False).iloc[0]

comparison = pd.DataFrame([
    {
        "Method": "PCA + KMeans (Baseline)",
        "Best k": int(best_baseline["k"]),
        "Silhouette": float(best_baseline["silhouette"]),
        "Calinski–Harabasz": float(best_baseline["calinski_harabasz"])
    },
    {
        "Method": "VAE + KMeans",
        "Best k": int(best_vae["k"]),
        "Silhouette": float(best_vae["silhouette"]),
        "Calinski–Harabasz": float(best_vae["calinski_harabasz"])
    }
])

# Save
comparison.to_csv("results/final_comparison.csv", index=False)

print("✅ Saved results/final_comparison.csv")
comparison

"""# Medium"""

import os
import numpy as np
import pandas as pd
import librosa

os.makedirs("data/audio_cnn", exist_ok=True)
os.makedirs("data/specs", exist_ok=True)

SR = 22050
DURATION = 10  # seconds
N_MELS = 64

df = pd.read_csv("data/metadata.csv")

specs = []
ids = []

for _, row in df.iterrows():
    track_id = row["track_id"]
    path = row["audio_path"]

    y, sr = librosa.load(path, sr=SR, mono=True)
    target_len = SR * DURATION
    # Fix: Replace librosa.util.fix_length with numpy operations
    if len(y) > target_len:
        y = y[:target_len]
    else:
        y = np.pad(y, (0, max(0, target_len - len(y))), 'constant')

    mel = librosa.feature.melspectrogram(
        y=y,
        sr=SR,
        n_mels=N_MELS
    )
    log_mel = librosa.power_to_db(mel, ref=np.max)

    specs.append(log_mel)
    ids.append(track_id)

X_spec = np.array(specs, dtype=np.float32)

np.save("data/specs/mel_specs.npy", X_spec)
pd.DataFrame({"track_id": ids}).to_csv("data/specs/track_ids.csv", index=False)

print("Saved mel spectrograms:", X_spec.shape)

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, random_split

os.makedirs("results_medium", exist_ok=True)

# -----------------------
# Config
# -----------------------
SEED = 42
BATCH_SIZE = 8          # small dataset + CNN => keep small
EPOCHS = 120
LR = 1e-3
LATENT_DIM = 16
BETA = 1.0              # KL weight

torch.manual_seed(SEED)
np.random.seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# -----------------------
# Load mel spectrograms
# shape: (N, 64, 431)
# -----------------------
X = np.load("data/specs/mel_specs.npy").astype(np.float32)
track_ids = pd.read_csv("data/specs/track_ids.csv")["track_id"].tolist()
N, H, W = X.shape
print("Loaded:", X.shape)

# -----------------------
# Normalize (global mean/std)
# IMPORTANT for stable CNN training
# -----------------------
mean = X.mean()
std = X.std() + 1e-8
Xn = (X - mean) / std

np.save("results_medium/spec_norm_mean.npy", np.array([mean], dtype=np.float32))
np.save("results_medium/spec_norm_std.npy", np.array([std], dtype=np.float32))

# Add channel dim: (N, 1, H, W)
Xn = Xn[:, None, :, :]
X_tensor = torch.from_numpy(Xn)

dataset = TensorDataset(X_tensor)

# Train/val split
n_total = len(dataset)
n_val = max(1, int(0.2 * n_total))
n_train = n_total - n_val
train_ds, val_ds = random_split(dataset, [n_train, n_val],
                                generator=torch.Generator().manual_seed(SEED))

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)

# -----------------------
# CNN-VAE
# -----------------------
class ConvVAE(nn.Module):
    def __init__(self, latent_dim=16):
        super().__init__()
        # Encoder (Conv2D)
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # -> (16, 32, ~216)
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> (32, 16, ~108)
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1), # -> (64, 8,  ~54)
            nn.ReLU(),
        )

        # Figure out flattened size dynamically using a dummy forward
        with torch.no_grad():
            dummy = torch.zeros(1, 1, H, W)
            enc_out = self.encoder(dummy)
            self.enc_shape = enc_out.shape[1:]         # (C, H', W')
            self.flat_dim = int(np.prod(self.enc_shape))

        self.fc_mu = nn.Linear(self.flat_dim, latent_dim)
        self.fc_logvar = nn.Linear(self.flat_dim, latent_dim)

        # Decoder
        self.fc_dec = nn.Linear(latent_dim, self.flat_dim)

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),
            # No activation here; we reconstruct normalized values (can be any real)
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        h = self.encoder(x)                   # (B, C, H', W')
        h_flat = h.view(x.size(0), -1)        # (B, flat_dim)
        mu = self.fc_mu(h_flat)
        logvar = self.fc_logvar(h_flat)
        z = self.reparameterize(mu, logvar)
        h_dec = self.fc_dec(z).view(x.size(0), *self.enc_shape)
        x_hat = self.decoder(h_dec)
        # Decoder output may be slightly larger due to transposed conv padding; crop to input size.
        x_hat = x_hat[:, :, :H, :W]
        return x_hat, mu, logvar

def vae_loss(x, x_hat, mu, logvar, beta=1.0):
    recon = nn.functional.mse_loss(x_hat, x, reduction="mean")
    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon + beta * kl, recon, kl

model = ConvVAE(latent_dim=LATENT_DIM).to(device)
opt = torch.optim.Adam(model.parameters(), lr=LR)

best_val = float("inf")
history = []

for epoch in range(1, EPOCHS + 1):
    model.train()
    tr_loss = tr_recon = tr_kl = 0.0

    for (xb,) in train_loader:
        xb = xb.to(device)
        opt.zero_grad()
        x_hat, mu, logvar = model(xb)
        loss, recon, kl = vae_loss(xb, x_hat, mu, logvar, beta=BETA)
        loss.backward()
        opt.step()

        tr_loss += loss.item() * len(xb)
        tr_recon += recon.item() * len(xb)
        tr_kl += kl.item() * len(xb)

    tr_loss /= n_train
    tr_recon /= n_train
    tr_kl /= n_train

    model.eval()
    va_loss = va_recon = va_kl = 0.0
    with torch.no_grad():
        for (xb,) in val_loader:
            xb = xb.to(device)
            x_hat, mu, logvar = model(xb)
            loss, recon, kl = vae_loss(xb, x_hat, mu, logvar, beta=BETA)
            va_loss += loss.item() * len(xb)
            va_recon += recon.item() * len(xb)
            va_kl += kl.item() * len(xb)

    va_loss /= n_val
    va_recon /= n_val
    va_kl /= n_val

    history.append([epoch, tr_loss, tr_recon, tr_kl, va_loss, va_recon, va_kl])

    if va_loss < best_val:
        best_val = va_loss
        torch.save(model.state_dict(), "results_medium/cnn_vae_model.pt")

    if epoch == 1 or epoch % 20 == 0:
        print(f"Epoch {epoch:03d} | train {tr_loss:.4f} (recon {tr_recon:.4f}, kl {tr_kl:.4f}) "
              f"| val {va_loss:.4f} (recon {va_recon:.4f}, kl {va_kl:.4f})")

hist_df = pd.DataFrame(history, columns=[
    "epoch","train_loss","train_recon","train_kl","val_loss","val_recon","val_kl"
])
hist_df.to_csv("results_medium/cnn_vae_training_log.csv", index=False)

print("✅ Saved results_medium/cnn_vae_model.pt and results_medium/cnn_vae_training_log.csv")
print("Best val loss:", best_val)

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import os

os.makedirs("results_medium", exist_ok=True)

# Must match training config
LATENT_DIM = 16

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load mel specs
X = np.load("data/specs/mel_specs.npy").astype(np.float32)
track_ids = pd.read_csv("data/specs/track_ids.csv")["track_id"].tolist()
N, H, W = X.shape

# Load normalization params
mean = float(np.load("results_medium/spec_norm_mean.npy")[0])
std  = float(np.load("results_medium/spec_norm_std.npy")[0])

Xn = (X - mean) / (std + 1e-8)
Xn = Xn[:, None, :, :]  # (N,1,H,W)
X_tensor = torch.from_numpy(Xn).to(device)

# Same ConvVAE definition as training (encoder+mu/logvar must match)
class ConvVAE(nn.Module):
    def __init__(self, latent_dim=16):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.ReLU(),
        )
        with torch.no_grad():
            dummy = torch.zeros(1, 1, H, W)
            enc_out = self.encoder(dummy)
            self.enc_shape = enc_out.shape[1:]
            self.flat_dim = int(np.prod(self.enc_shape))

        self.fc_mu = nn.Linear(self.flat_dim, latent_dim)
        self.fc_logvar = nn.Linear(self.flat_dim, latent_dim)

    def forward(self, x):
        h = self.encoder(x)
        h_flat = h.view(x.size(0), -1)
        mu = self.fc_mu(h_flat)
        logvar = self.fc_logvar(h_flat)
        return mu, logvar

model = ConvVAE(latent_dim=LATENT_DIM).to(device)
state = torch.load("results_medium/cnn_vae_model.pt", map_location=device)
model.load_state_dict(state, strict=False)
model.eval()

with torch.no_grad():
    mu, _ = model(X_tensor)
    Z_audio = mu.cpu().numpy()

np.save("results_medium/z_audio_cnn.npy", Z_audio)
pd.DataFrame(Z_audio, columns=[f"z{i}" for i in range(Z_audio.shape[1])]).assign(track_id=track_ids) \
  .to_csv("results_medium/z_audio_cnn.csv", index=False)

print("✅ Saved results_medium/z_audio_cnn.npy and results_medium/z_audio_cnn.csv")
print("z_audio_cnn shape:", Z_audio.shape)

from google.colab import files
uploaded = files.upload()

import os, shutil

os.makedirs("/root/.kaggle", exist_ok=True)
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 0o600)
print("✅ Kaggle API configured")

!pip -q install kaggle
!kaggle datasets download -d andradaolteanu/jamendo-music-dataset -p data/jamendo --unzip

import os

for root, dirs, files in os.walk("data/jamendo"):
    if len(files) > 0:
        print(root, "->", len(files), "files")
        print("   sample:", files[:5])
        break

import glob
csvs = glob.glob("data/jamendo/**/*.csv", recursive=True)
csvs[:20], len(csvs)

import pandas as pd

if not csvs:
    print("Error: No CSV files found. Please ensure the Kaggle dataset downloaded successfully.")
    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors
else:
    df = pd.read_csv(csvs[0])  # change [0] to the correct index if needed
    print(df.shape)
    df.head()

import pandas as pd
import os

os.makedirs("data", exist_ok=True)

# Replace these with the actual column names you find in Jamendo CSV
AUDIO_COL = "audio_path"   # <-- change
LYRICS_COL = "lyrics"      # <-- change

if not csvs:
    print("Error: No CSV files found. Cannot process metadata or lyrics.")
    # Exit or handle gracefully if no CSVs are found
    # For now, we'll create empty dataframes to prevent further errors
    meta = pd.DataFrame(columns=["track_id", "audio_path", "language"])
    lyrics = pd.DataFrame(columns=["track_id", "lyrics"])
else:
    df = pd.read_csv(csvs[0])  # <-- change if needed

    # Keep only rows with audio + lyrics
    df = df.dropna(subset=[AUDIO_COL, LYRICS_COL]).copy()

    # Create track_id
    df["track_id"] = [f"{i+1:03d}" for i in range(len(df))]

    # Build metadata.csv
    meta = df[["track_id", AUDIO_COL]].rename(columns={AUDIO_COL: "audio_path"})
    meta["language"] = "Unknown"

    # Build lyrics.csv
    lyrics = df[["track_id", LYRICS_COL]].rename(columns={LYRICS_COL: "lyrics"})

meta.to_csv("data/metadata.csv", index=False)
lyrics.to_csv("data/lyrics.csv", index=False)

print("✅ Saved data/metadata.csv and data/lyrics.csv")
print("metadata rows:", len(meta))

import pandas as pd

meta = pd.read_csv("data/metadata.csv")
lyrics = pd.read_csv("data/lyrics.csv")

N = min(300, len(meta))  # choose 300 max
meta = meta.head(N)
lyrics = lyrics.head(N)

meta.to_csv("data/metadata.csv", index=False)
lyrics.to_csv("data/lyrics.csv", index=False)

print("✅ Using subset size:", N)

df.columns.tolist()
